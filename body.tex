\section{Introduction}

Phylogenetics is rapidly progressing as the statistical foundation of
comparative biology, providing a framework that accounts for the shared
ancestry inherent in biological data.
Soon after phylogenetics became feasible as a likelihood-based statistical
endeavor \citep{Felsenstein1981}, models became richer to better
capture processes of biological diversification and character change.
This increasing trend in model complexity made Bayesian approaches appealing,
because they can approximate posterior distributions of rich models by
leveraging prior information and numerical techniques (e.g., data
augmentation).
\vmcomment{``data augmentation'' looks strange to me here, because 1) you can do MLE with EM, which is also a data augmentation technicque, so this concept is not exclusively Bayesian; 2) it is a bit weird to talk about data augmentation before MCMC. At the risk of trying to read your mind, I'd suggest replacing ``numerical techniques'' with ``hierarchical models, where researchers can take into account uncertaintly of all levels/layers in the hierarchy.''}

From the earliest days of Bayesian phylogenetics \citep{Rannala1996,Mau1997},
the numerical tool of choice for approximating the posterior distribution was
Markov chain Monte Carlo (MCMC).
The popularity of MCMC was due, in no small part, to avoiding the calculation
of the marginal likelihood of the model---the probability of the data under the
model, averaged, with respect to the prior, over the whole parameter space.
This marginalized measure of model fit \vmdel{cannot be solved analytically} \vmadd{is not easy to compute} due to the
large number of parameters in phylogenetic models over which the likelihood
needs to be summed or integrated. 

Marginal likelihoods are central to model comparison in a Bayesian framework.
Thus, we cannot avoid calculating them if we want to be able to compare the fit
of phylogenetic models.
As the diversity and richness of phylogenetic models has increased, there has
been a renewed appreciation of the importance of such Bayesian model
comparison.
As a result, there has been substantial work over the last decade to develop
methods for estimating marginal likelihoods of phylogenetic models.

The goals of this review are to
(1) provide a summary about what marginal likelihoods are and why they are
useful,
(2) review the various methods available for approximating marginal likelihoods
of phylogenetic models,
(3) review some of the ways marginal likelihoods have been applied to learn
about evolutionary history and processes,
(4) discuss some of the challenges of Bayesian model choice, and
(5) highlight some promising avenues for advancing the field of Bayesian
phylogenetics.

\section{What is a marginal likelihood?}

\begin{linenomath}
A marginal likelihood is the average fit of a model to a dataset.
More specifically, it is an average over the entire parameter space of the
likelihood weighted by the prior.
For a phylogenetic model \model with parameters that include the discrete
topology (\topology) and continuous branch lengths and other parameters that
govern the evolution of the characters along the tree (together represented by
\evoparameters), the marginal likelihood can be represented as
\begin{equation}
    p(\data \given \model) =
    \sum\limits_{\topology}
    \int_{\evoparameters}
    p( \data \given \topology, \evoparameters, \model)
    p(\topology, \evoparameters \given \model)
    \diff{\evoparameters},
    \label{eq:marginalLikelihood}
\end{equation}
where \data are the data.
Each parameter of the model adds a dimension to the model, over which the
likelihood must be averaged.
\end{linenomath}

\begin{linenomath}
The marginal likelihood is also the proportionality constant in the denominator
of Bayes' rule that ensures the posterior is a proper probability density that
sums and integrates to one:
\begin{equation}
    p(\topology, \evoparameters \given \data, \model) = \frac{
        p(\data \given \topology, \evoparameters, \model)
        p(\topology, \evoparameters \given \model)
    }{
        p(\data \given \model)
    }.
    \label{eq:bayesRule}
\end{equation}
\end{linenomath}

\section{Why are marginal likelihoods useful?}

Marginal likelihoods are the currency of model comparison in a Bayesian
framework. The frequentist approach to model choice is based on comparing
the maximum probability of the data under two models either using
a likelihood ratio test or some information-theoretic criterion.
Because adding a parameter (dimension) to a model will always ensure
a maximum likelihood at least as large as without the parameter, some
penalty must be imposed when parameters are added.

In a Bayesian perspective, we are interested in comparing the average fit of a
model, rather than the maximum.
This imposes a ``natural'' penalty for parameters, because each additional
parameter introduces a dimension that must be averaged over.
If that dimension introduces substantial parameter space with small likelihood,
and little space that improves the likelihood, it will decrease the marginal
likelihood.
Thus, unlike the maximum likelihood, adding a parameter to a model can
decrease the \emph{marginal} likelihood.

\begin{linenomath}
The ratio of two marginal likelihoods gives us the factor by which the
average fit of the model in the numerator is better or worse than the
model in the denominator.
This is called the Bayes factor \citep{Jeffreys1935}.
We can again leverage Bayes' rule to gain more intuition for how marginal
likelihoods and Bayes factors guide Bayesian model selection by
writing it in terms of the posterior probability of a model, \model[1],
among \nmodels candidate models:
\begin{equation}
    p(\model[1] \given \data) = \frac{
        p(\data \given \model[1])
        p(\model[1])
    }{
        \sum\limits_{i=1}^{\nmodels}
        p(\data \given \model[i])
        p(\model[i])
    }.
    \label{eq:bayesRuleModelProbability}
\end{equation}
This shows us that the posterior probability of a model is proportional to the
prior probability multiplied by the marginal likelihood of that model.
Thus, the marginal likelihood is how the data update our prior beliefs about a
model.
As a result, it is often simply referred to as ``the evidence''
\citep{MacKay2005}.
If we look at the ratio of the posterior probabilit\vmadd{ies}\vmdel{y} of two models,
\begin{equation}
    \frac{
        p(\model[1] \given \data)
    }{
        p(\model[2] \given \data)
    }
    =
    \frac{
        p(\data \given \model[1])
    }{
        p(\data \given \model[2])
    }
    \times
    \frac{
        p(\model[1])
    }{
        p(\model[2])
    },
    \label{eq:modelOdds}
\end{equation}
we see that the Bayes factor is the factor by which the prior odds of a model
is multiplied to give use the posterior odds.
Thus, marginal likelihoods and their ratios give us intuitive measures of how
much the data ``favor'' one model over another, and these measures \vmdel{are solidly
grounded in probability theory} \vmadd{have natural probabilistic interpretations}.
\end{linenomath}

\section{An example of coin flipping}

To help gain some intuition for marginal likelihoods and how they differ from
the posterior distribution of a model, let's us a simple, albeit contrived,
example.
Let's assume we are interested in the probability of a coin we have not seen
landing heads-side up when it is flipped (\probheads).
Our plan is to flip this coin 100 times and count the number of times it lands
heads up, which we model as a random outcome from a binomial distribution.
Before flipping, we decide to compare four models that vary in our
prior assumptions about the probability of the coin landing heads up:
We assume
\begin{enumerate}
    \item all values are equally probable
        (\coinmodel[1]: $\probheads \sim \textrm{Beta}(1, 1)$),
    \item the coin is likely weighted to land mostly ``heads'' or ``tails''
        (\coinmodel[2]: $\probheads \sim \textrm{Beta}(0.6, 0.6)$),
    \item the coin is probably fair
        (\coinmodel[3]: $\probheads \sim \textrm{Beta}(5.0, 5.0)$), and
    \item the coin is weighted to land tails side up most of time
        (\coinmodel[4]: $\probheads \sim \textrm{Beta}(1.0, 5.0)$).
\end{enumerate}

\begin{linenomath}
After flipping the coin and observing that it landed heads side up 50 times,
we can calculate the posterior probability distribution for the probability
of landing heads up under each of our four models:
\begin{equation}
    p(\probheads \given \flipdata, \coinmodel[i]) = \frac{
        p(\flipdata \given \probheads, \coinmodel[i]) p(\probheads \given \coinmodel[i])
    }{
        p(\flipdata \given \coinmodel[i])
    }
    \label{eq:coinBayesRule}
\end{equation}
Doing so, we see that regardless of our prior assumptions about the
probability of the coin landing heads, the posterior distribution is
very similar (Figure~\ref{fig:bayesDemo}).
This makes sense; given we observed 50 heads out of 100 flips, values for
\probheads toward zero and one are extremely unlikely, and the posterior is
dominated by the likelihood of values near 0.5.
\end{linenomath}

\begin{linenomath}
Given the posterior distribution for \probheads is very robust to our prior
assumptions, we might assume that each of our four models explain the data
similarly well.
However, to compare their ability to explain the data, we need to
average (integrate) the likelihood density function over all possible
values of \probheads, weighting by the prior:
\begin{equation}
    p(\data \given \model[i]) =
    \int_{\probheads}
    p( \data \given \probheads, \model[i])
    p(\probheads \given \model[i])
    \diff{\probheads},
    \label{eq:coinMarginalLikelihood}
\end{equation}
Looking at the plots in Figure~\ref{fig:bayesDemo} we see that the models that
place a lot of prior weight on values of \probheads that do not explain the
data well (i.e., have small likelihood) have a much smaller marginal likelhood.
Thus, even if we have very informative data that make the posterior
distribution robust to prior assumptions, this example illustrates that the
marginal likelihood of model can still be very sensitive to the prior
assumptions we make about the parameters.
\end{linenomath}


\section{Methods for marginal likelihood approximation}

For all but the simplest of models, the summation and integrals in
Equation~\ref{eq:marginalLikelihood}
are analytically intractable.
This is particularly true for phylogenetic models, which have a complex
structure containing both discrete and continuous elements \citep{Kim2000}.
Thus, we must resort to numerical techniques to approximate the marginal
likelihood.

Perhaps the simplest numerical approximation of the marginal likelihood is to
draw samples of a model's parameters from their respective prior distributions.
This turns the intractable integral into a sum of the samples' likelihoods.
Because the prior weight of each sample is one in this case, the marginal
likelihood can be approximated by simply calculating the average likelihood of
the prior samples.
Similarly, if we have a sample of the parameters from the posterior
distribution---like one obtained from a ``standard'' Bayesian phylogenetic
analysis via MCMC---we can again use summation to approximate the integral.
In this case, the weight of each sample is the ratio of the prior density to
the posterior density.
As a result, the sum simplifies to the harmonic mean (HM) of the likelihoods
from the posterior sample \citep{Newton1994}.
Both of these techniques are importance-sampling approximations, and suffer
from the fact that the prior and posterior are often \emph{very} divergent,
with the latter usually \emph{much} more peaked than the former.
A finite sample from the prior will often yield an underestimate of the
marginal likelihood, because the region of parameter space with high likelihood
is likely to be missed.
Whereas a finite sample from the posterior will almost always lead to an
overestimate \citep{Lartillot2006,Xie2011,Fan2011}, because it will contain
very few samples outside of the region of high likelihood, where the prior has
a strong downward ``pull'' on the average likelihood.

Recent methods developed to estimate marginal likelihoods generally fall into
two categories for dealing with the sharp contrast between the prior
and posterior that cripples the simple importance-sampling approaches
mentioned above.
One general strategy is to turn the giant leap between the unnormalized
posterior and prior into many small steps.
% along a series of power-posterior distributions.
The second strategy is to turn the giant leap between the posterior and prior
into a smaller leap between the posterior and a reference distribution that is
as similar as possible to the posterior.
These approaches are not mutually exclusive (e.g., see Fan et al.\
(\citeyear{Fan2011})), but they serve as a useful way to categorize many of the
methods available for approximating marginal likelihoods.
In practical terms, the first strategy is computationally expensive, because
samples need to be collected from each step between the posterior and prior,
which is not normally part of a standard Bayesian phylogenetic analysis.
The second strategy is very inexpensive because it attempts to approximate the
marginal likelihood using only the posterior samples collected from a
typical analysis.

\subsection{Approaches that bridge the prior and posterior with small steps}

\subsubsection{Path sampling (PS)}
Lartillot and Philippe (\citeyear{Lartillot2006}) introduced path sampling
(also called thermodynamic integration) to phylogenetics to address the problem
that the posterior is often dominated by the likelihood and very divergent from
the prior.
Rather than restrict themselves to a sample from the posterior, they collected
MCMC samples from a series of distributions between the prior and posterior.
Specifically, samples are taken from a series of power-posterior distributions,
where the likelihood is raised to a power $\beta$:
$ p(\data \given \topology, \evoparameters, \model)^{\beta}
p(\topology, \evoparameters \given \model)$.
When $\beta = 1$, this is equal to the unnormalized joint posterior, which
integrates to what we want to know, the marginal likelihood.
When $\beta = 0$, this is equal to the joint prior distribution, which,
assuming we are using proper prior probability distributions, integrates to 1.
If we integrate the power posterior over 0--1 with respect to $\beta$, we get
the ratio of the normalizing constants when $\beta$ equals 1 and 0, and since
we know the constant is 1 when $\beta$ is zero, we are left with the marginal
likelihood.
Lartillot and Philippe (\citeyear{Lartillot2006}) approximated this integral by
summing over MCMC samples taken from a discrete number of $\beta$ values evenly
distributed between 1 and 0.

\subsubsection{Stepping stone (SS)}
The stepping-stone method introduced by Xie et al.\ (\citeyear{Xie2011})
is similar to PS in that it also uses samples from power posteriors, but the
motivation is not based on approximating the integral per se, but by
the fact that we can very accurately use importance sampling to approximate the
ratio of normalizing constants at each step between the posterior and prior.
Also, Xie et al.\ (\citeyear{Xie2011}) chose the values of $\beta$ for the
series of power posteriors from which to sample so that most were close to
the prior (reference) distribution, rather than evenly distributed between
0 and 1.
This is beneficial, because most of the change happens near the prior; the
likelihood begins to dominate quickly, even at small values of $\beta$.
The stepping-stone method results in more accurate estimates of the marginal
likelihood with fewer steps than PS \citep{Xie2011}.

\subsubsection{Generalized stepping stone (GSS)}
The most accurate estimator of marginal likelihoods available to date, the
generalized stepping-stone (GSS) method, leverages both strategies; taking many
small steps from a starting point (reference distribution) that is much closer
to the posterior than the prior \citep{Fan2011}.
Fan et al.\ (\citeyear{Fan2011}) improved upon the original stepping-stone
method by using a reference distribution that, in most cases, will be much more
similar to the posterior than the prior.
The reference distribution has the same form as the joint prior, but each
marginal prior distribution is adjusted so that its mean and variance matches
the corresponding sample mean and variance of an MCMC sample from the
posterior.
This guarantees that the support of the reference distribution will cover the
posterior.
Initially, the application of the GSS method was limited, because it required
that the topology be fixed, because there was no reference distribution across
topologies.
However, Holder et al.\ (\citeyear{Holder2014}) introduced such a distribution
on trees, allowing the GSS to approximate the fully marginalized likelihood of
phylogenetic models.

\subsubsection{Sequential Monte Carlo (SMC)}
Another approach that uses sequential importance-sampling steps is sequential
Monte Carlo (SMC), also known as particle filtering.
Recently, SMC algorithms have been developed for approximating the posterior
distribution of phylogenetic trees \citep{Jordan2012,Bouchard2014}.
While inferring the posterior, SMC algorithms can approximate the marginal
likelihood of the model ``for free,'' by keeping a running average of the
importance-sampling weights of the trees (particles) along the way.
SMC algorithms hold a lot of promise for complementing MCMC in Bayesian
phylogenetics due to their sequential nature and ease with which the
computations can be parallelized
\citep{Jordan2012,Dinh2016preprint,Fourment2017preprint}.
However, these approaches are still in their infancy, and the accuracy of SMC
estimates of marginal likelihoods still need to be compared to the methods
discussed above.
See Bouchard-C\^{o}t\'{e} (\citeyear{Bouchard2014}) for an accessible treatment
of SMC in phylogenetics.

\subsubsection{Nested sampling (NS)}
Recently, Maturana Patricio et al.\ (\citeyear{Maturana2017})
introduced the numerical technique known as nested sampling to Bayesian
phylogenetics.
% One recent method that does not fit into our dichotomy of using power-posterior
% samples versus posterior samples, is the use of nested sampling by Maturana
% Patricio et al.\ (\citeyear{Maturana2017}.)
\thought{Check most appropriate family name to use for Patricio Andres Maturana
    Russel. website lists other publications as: Maturana Patricio. Maybe use: Maturana Patricio, P. A.}
This tries to simplify the multi-dimensional integral in
Equation~\ref{eq:marginalLikelihood}
into a one-dimensional integral over the cumulative distribution function
of the likelihood.
The latter can be numerically approximated using basic quadrature methods,
essentially summing up the area of polygons under the likelihood function.
The algorithm works by starting with a random sample of parameter values
from the joint prior distribution and their associated likelihood
scores.
Sequentially, the sample with the lowest likelihood is removed and replaced by
a another random sample from the prior with the constraint that its likelihood
must be larger than the removed sample.
The approximate marginal likelihood is a running sum of the likelihood of these
removed samples with appropriate weights.
Re-sampling these removed samples according to their weights yields a posterior
sample at no extra computational cost.
Initial assessment of NS suggest it performs similarly to GSS.
As with SMC, NS seems like a promising complement to MCMC for both
approximating the posterior and marginal likelihood of phylogenetic models.


\subsection{Approaches that use only posterior samples}

\subsubsection{Generalized harmonic mean (GHM)}
Gelfand and Dey (\citeyear{Gelfand1994}) introduced a generalized harmonic mean
estimator that uses an arbitrary normalized reference distribution, as
opposed to the prior distribution used in the HM estimator, to weight the
samples from the posterior.
If the chosen reference distribution is more similar to the posterior than the
prior (i.e., a ``smaller leap'' as discussed above), the GHM estimator will
perform better than the HM estimator.
However, for high-dimensional phylogenetic models, choosing a suitable
reference distribution is very challenging, especially for tree topologies.
As a result, the GHM estimator has not been used for comparing phylogenetic
models.
However, recent advances on defining a reference distribution on trees
\citep{Holder2014} makes the GHM a tenable option in phylogenetics.
\thought{I think that Matsen might also have some work on tree distributions.}

\subsubsection{Inflated-density ratio (IDR)}
The inflated-density ratio estimator solves the problem of choosing a
reference distribution by using a perturbation of the posterior density;
essentially the posterior is ``inflated'' from the center by a known radius
\citep{Petris2007,Arima2012,Arima2014}.
As one might expect, the radius must be chosen carefully.
The application of this method to phylogenetics has been limited by the fact
that all parameters must be unbounded; any parameters that are bounded (e.g.,
must be positive) must be re-parameterized to span the real number line.
As a result, this method cannot be applied directly to MCMC samples collected
by popular Bayesian phylogenetic software packages.
Nonetheless, the IDR estimator has recently been applied to phylogenetic models
\citep{Arima2014}, including in settings where the topology is allowed to vary
\citep{Wu2014}.
Initial applications of the IDR are very promising, demonstrating comparable
accuracy to methods that sample from power-posterior distributions while
avoiding such computation \citep{Arima2014,Wu2014}.
Currently, however, the IDR has only been used on relatively small datasets and
simple models of character evolution.
More work is necessary to determine whether the promising combination of
accuracy and computational efficiency holds for large datasets and rich models.

\subsubsection{Partition-weighted kernal (PWK)}
Recently, Wang et al.\ (\citeyear{Wang2017}) introduced the partition weighted
kernal (PWK) method of approximating marginal likelihoods.
This approach entails partitioning parameter space into regions within which
the posterior density is relatively homogeneous.
Given the complex structure of phylogenetic models, it is not obvious how this
would be done.
As of yet, this method has not been used for phylogenetic models.
However, for simulations of mixtures of bivariate normal distributions, the
PWK outperforms the IDR estimator \citep{Wang2017}.
Thus, if this method is adapted to phylogenetic models, it is very promising.


\subsection{Bayesian model averaging}

An alternative to using marginal likelihoods for Bayesian model comparison is
to sample across the competing models directly.
The frequency of samples from each model approximates their posterior
probability, which can be used to approximate Bayes factors among models.
Algorithms for sampling across models include reversible-jump MCMC
\citep{Green1995}, Gibbs sampling \citep{Neal2000}, Bayesian stochastic search
variable selection \citep{George1993,Kuo1998}, and approximations of reversible-jump
\citep{Jones2015}.
In terms of selecting the correct ``relaxed-clock'' model from simulated data,
Baele and Lemey (\citeyear{Baele2014}) showed that model-averaging performed
similarly to the path-sampling and stepping-stone marginal likelihood
estimators.

There are a couple of limitations for these approaches.
First, a Bayes factor that includes a model with small posterior probability
will suffer from Monte Carlo error.
For example, unless a very large sample from the posterior is collected, some
models might not be sampled at all.
Second, and perhaps more importantly, for these numerical algorithms to be able
to ``jump'' among models, the models being sampled need to be similar.

If two highly dissimilar models need to be compared,
Lartillot and Philippe (\citeyear{Lartillot2006}) introduced a method of using
path sampling to directly approximate the Bayes factor.
Similarly, Baele et al.\ (\citeyear{Baele2013}) extended the stepping-stone
approach of Xie et al.\ (\citeyear{Xie2011}) to do the same.
However, if there are many models to compare, doing MCMC over power posteriors
for every pairwise comparison will quickly become computationally prohibitive;
approximating the marginal likelihood of each model would be simpler.

\subsection{Approximate-likelihood approaches}

Approximate-likelihood Bayesian computation (ABC) approaches
\citep{Tavare1997,Beaumont2002} have become popular in situations where it is
not possible (or undesirable) to derive and compute the likelihood function of
a model.
The basic idea is simple: by generating simulations under the model, the
fraction of times that we generate a simulated dataset that matches the
observed data is a Monte Carlo approximation of the likelihood.
Because, simulating the observed data exactly is often not possible (or extremely
unlikely), simulations ``close enough'' to the observed data are
counted, and usually a set of insufficient summary statistics are used in place
of the data.
Whether a simulated dataest is ``close enough'' to count is formalized as
whether or not it falls within a zone of tolerance around the empirical data.

This simple approach assumes the likelihood within the zone of tolerance is
uniform.
However, this zone usually needs to be quite large for computational
tractability, so this assumption does not hold.
Leuenberger and Wegmann \citep{Leuenberger2010} proposed fitting a general
linear model (GLM) to approximate the likelihood within the zone of tolerance.
With the GLM in hand, the marginal likelihood of the model can simply be
approximated by the marginal density of the GLM.

The accuracy of this estimator has not been assessed.
However, there are good theoretical reasons to be skeptical of its accuracy.
Because the GLM is only fit within the zone of tolerance (also called the
``truncated prior''), it cannot account for the weight of the prior on the
marginal likelihood outside of this region.
Whereas the posterior distribution usually is not strongly influenced by
regions of parameter space with low likelihood, the marginal likelihood very
much is.
This is analogous to the harmonic mean being a poor estimator of the marginal
likelihood due to having very few samples outside of the region of high
likelihood, where the prior has a strong downward ``pull'' on the average
likelihood.

To test these predictions, we assessed the behavior of the ABC-GLM method on
100 datasets simulated under the simplest possible phylogenetic model: two DNA
sequences separated by a single branch along which the sequence evolved under a
Jukes-Cantor model of nucleotide substitution \citep{JC1969}.
The simulated sequences were 10,000 nucleotides long, and the prior on the only
parameter in the model, the length of the branch, was a uniform distribution
from 0.0001 to 0.1 substitutions per site.
For such a simple model, we used quadrature integration to calculate the
``true'' marginal likelihood for each simulated alignment of two sequences.
Integration using 1,000 and 10,000 steps and rectangular and trapezoidal
quadrature rules all yielded identical values for the log marginal likelihood
to at least five decimal places for all 100 simulated data sets, providing a
very precise proxy for the true values.
A sufficient summary statistic, the proportion of variable sites, was used for
ABC analyses.
However, the ABC-GLM and quadrature marginal likelihoods are not directly
comparable, because the marginal probability of the proportion of variable
sites versus the site pattern counts will be on different scales that are data
set dependent.
So, we compare the ratio of marginal likelihoods (i.e., Bayes factors)
comparing the correct branch-length model
[branch length $\sim$ uniform(0.0001, 0.1)]
to a model with a prior approximately twice as broad
[branch length $\sim$ uniform(0.0001, 0.2)].

This very simple model is a good test of the ABC-GLM marginal likelihood
estimator for several reasons.
The use of a sufficient statistic for a finite, one-dimensional model makes ABC
nearly equivalent to a full-likelihood Bayesian method
(Figure~S\ref{fig:branchLengthEstimates}).
Thus, this is a ``best-case scenario'' for the ABC-GLM approach.
Also, we can use quadrature integration for very good proxies for the true
Bayes factors.
Lastly, the simple scenario gives us some analytical expectations for the
behavior of ABC-GLM.
If it cannot penalize the marginal likelihood for the additional branch length
space in the model with the broader prior, the Bayes factor should be off by a
factor of approximately 2, or more precisely $(0.2-0.0001) / (0.1-0.0001)$.
As shown in Figure~\ref{fig:glmPerformance}, this is exactly what we find.
This confirms our prediction that the ABC-GLM approach cannot average over
regions of parameter space with low likelihood and thus will be biased toward
favoring models with more parameter space.
Given that the GLM approximation of the likelihood is only fit within a subset
of parameter space with high likelihood, which is usually a \emph{very} small
region of a model, the marginal of the GLM should not be considered a marginal
likelihood of the model.

Full details of these analyses, which were all designed atop the DendroPy
phylogenetic API (version 4.3.0 commit 72ce015) \citep{Sukumaran2010}, can be
found in the supplementary materials, and all of the code to replicate our
results is freely available at
\href{https://github.com/phyletica/abc-glm-marginal-test}{https://github.com/phyletica/abc-glm-marginal-test}.


\section{Uses of marginal likelihoods}

\thought{We can't be comprehensive here. Subsections need to be integrated to
    highlight how we can use marginal likelihoods to learn about evolution}

\subsection{Comparing partitioning schemes}

One of the earliest applications of marginal likelihoods in phylogenetics was
to choose among ways of assigning models of substitution to different subsets
of aligned sites.
This became important when phylogenetics moved beyond singe-gene trees to
concatenated alignments of several genes.
Mueller et al. (\citeyear{Mueller2004}),
Nylander et al.\ (\citeyear{NylanderEtal2004}), and
Brandley et al.\ (\citeyear{Brandley2005})
used Bayes factors calculated from harmonic mean
estimates of marginal likelihoods to choose among different strategies for
partitioning aligned characters to subtitution models.

All three studies found that the model with the most subsets was strongly
preferred.
Nylander et al.\ (\citeyear{NylanderEtal2004}) also showed that removing
parameters for which the data seemed to have little influence decreased the HM
estimates of the marginal likelihood, suggesting that the HM estimates might
favor over-parameterized models.
These findings could be an artefact of the bias of the HM estimator toward
overestimating marginal likelihoods and underestimating the ``penalty''
associated with the prior weight of additional parameters.
However, Brown and Lemmon (\citeyear{Brown2007}) showed that for simulated data,
HM estimates of Bayes factors can have a low error rate of over-partitioning an
alignment.

Fan et al.\ (\citeyear{Fan2011}) show that, again, the HM estimator strongly
favors the most partitioned model for a four-gene alignment from cicadas (12
subsets partitioned by gene and codon position).
However, the marginal likelihoods estimated via the generalized stepping stone
method favor a much simpler model (3 subsets partitioned by codon position).
This demonstrates the bias inherent in simple importance-sampling methods
for estimating the average likelihood of phylogenetic models.
It also suggests that relatively few, well-assigned subsets can go a long way
to explain the variation in substitution rates among sites.

\subsection{Comparing models of character substitution}

Lartillot and Philippe (\citeyear{Lartillot2006}) used path sampling to compare
models of amino-acid substitution.
They found that the harmonic mean estimator favored the most parameter rich
model for all five datasets they explored, whereas the path-sampling estimates
favored simpler models for three of the datasets.
This again demonstrates that accurately estimated marginal likelihoods do
indeed ``penalize'' for over-parameterization of phylogenetic models.
More importantly, this work also revealed that modeling pattern heterogeneity
across sites of an alignment better explains the variation in biological data.

\subsection{Comparing ``relaxed clock'' models}

Lepage et al.\ (\citeyear{Lepage2007}) used path sampling to approximate Bayes
factors comparing various ``relaxed-clock'' phylogenetic models for
three empirical datasets.
They found that models in which the rate of substitution evolves across the tree
(autocorrelated rate models) better explain the empirical sequence alignments
they investigated than models that assume the rate of substitution on each
branch is independent (uncorrelated rate models).

Baele et al.\ (\citeyear{Baele2012}) used marginal likelihoods to demonstrate
the importance of using sampling dates when estimating time-calibrated
phylogenetic trees.
They used path-sampling and stepping-stone methods to estimate the marginal
likelihoods of strict and relaxed-clock models for sequence data of herpes
viruses.
They found that when the dates the viruses were sampled were provided, a strict
molecular clock was the best fit model, but when the dates were excluded,
relaxed-clock models were strongly favored.

Baele et al.\ (\citeyear{Baele2012}) also demonstrated that modelling
among-branch rate variation with a lognormal distribution tends to explain
mammalian sequence alignments better than using an exponential distribution.
They used marginal likelihoods (PS and SS estimates) and Bayesian model
averaging to compare the fit of lognormally and exponentially distributed
relaxed clocks for almost 1,000 loci from 12 mammalian species.
They found that the lognormal relaxed-clock was a better fit for almost 88\% of
the loci.

\subsection{Comparing demographic models}

Baele et al.\ (\citeyear{Baele2012}) used the path-sampling and stepping-stone
estimators for marginal likelihoods to compare the fit of various demographic
models to the HIV-1 group M data of Worobey et al.\ (\citeyear{Worobey2008}),
and Methicillin-resistant \spp{Staphylococcus aureus} (MRSA)  data of Gray et
al.\ (\citeyear{Gray2011}).
They found that a flexible, nonparametric model that enforces no particular
demographic history is a better explanation of the HIV and MRSA sequence data
than exponential and logistic population growth models.
This suggests that traditional parametric growth models are not the best
predictors of viral and bacterial epidemics.

% \subsection{Comparing tree models (priors)}

% I'm pretty sure this has been done, but I cannot find the paper.

\subsection{Phylogenetic factor analysis}

The goal of comparative biology is to understand the relationships among a
potentially large number of phenotypic traits across organisms. 
To do so correctly, the inherent shared ancestry underlying all life needs to
be accounted for \citep{Felsenstein1985PIC}.
A lot of progress has been made for inferring the relationship between pairs
of phenotypic traits as they evolve across a phylogeny,
but a general and efficient solution for large numbers of continuous
and discrete traits has remained elusive.
Tolkoff et al.\ (\citeyear{Tolkoff2017}) introduce Bayesian factor analysis to
a phylogenetic framework as a potential solution.
Phylogenetic factor analysis works by modeling a small number of unobserved
(latent) factors that evolve independently across the tree, which give rise to
the large number of observed continuous and discrete phenotypic traits.
This allows correlations among traits to be estimated, without having to model
every trait as a conditionally independent process.

The question that immediately arises is, what number of factors best explains
the evolution of the observed traits?
To address this, \citep{Tolkoff2017} use path sampling to approximate the
marginal likelihood of models with different numbers of traits.
To do so, they extend the path sampling method to handle the latent variables
underlying the discrete traits by softening the thresholds that delimit the
discrete character states across the series of power posteriors.
% This new approach demonstrates the power of marginal likelihoods for
% Bayesian model comparison.
This new approach leverages Bayesian model comparison via marginal likelihoods
to learn about the processes governing the evolution of multidimensional
phenotypes.


\subsection{Comparing phylogeographic models}

Phylogeographers are interested in explaining the genetic variation within and
among species across a landscape.
As a result, we are often interested in comparing models that include
various combinations of micro and macro-evolutionary processes and geographic
and ecological parameters.
Deriving the likelihood function for such models is often difficult and, as a
result, approximate-likelihood Bayesian computation (ABC) approaches to model
choice are often used.

At the forefront of generalizing phylogeographic models is an approach that is
often referred to as iDDC, which stands for integrating distributional,
demographic, and coalescent models \citep{Papadopoulou2016}.
This approach simulates data under various phylogeographical models upon
proxies for habitat suitability derived from species distribution models.
To choose the model the best explains the empirical data, this approach
uses the marginal densities of the models estimated via the ABC-GLM method
and p-values derived from these densities
\citep{He2013}
\citep{Massatti2016}
\citep{Bemmels2016}
\citep{Knowles2017}
\citep{Papadopoulou2016}.
This approach is an important step forward for bringing more biological realism
into phylogeographical models.
However, given that the marginal GLM densities should not be considered as
marginal likelihoods (see above; Figure~\ref{fig:glmPerformance}), these
methods should be seen as a useful exploration of data, rather than rigorous
hypothesis tests.
Because ABC-GLM marginal densities fail to penalize parameters for their
prior weight in regions of low likelihood, these approaches will likely
be biased toward over-parameterized phylogeographical models.
Nonetheless, knowledge of this bias can help guide interpretations of results.


\subsection{Species delimitation}
Calculating the marginal probability of sequence alignments \citep{Grummer2013}
and single-nucleotide polymorphisms \citep{Leache2014} under various
multi-species coalescent models has been used to estimate species boundaries.
By comparing the marginal likelihoods of models that differ in how they assign
individual organisms to species, systematists can calculate Bayes factors to
determine how much the genetic data support different delimitations.
Using simulated data, \citep{Grummer2013} found that marginal likelihoods
calculated using path sampling and stepping-stone methods outperformed harmonic
mean estimators at identifying the true species delimitation model.
Marginal likelihoods seem better able to distinguish some species delimitation
models than others.
For example, models that lump species together or reassign samples into
different species produce larger marginal likelihood differences versus models
that split populations apart \citep{Grummer2013, Leache2014}.
Current implementations of the multi-species coalescent assume strict models of
genetic isolation, and oversplitting populations that exchange genes creates a
difficult Bayesian model comparison problem that does not include the correct
model (Leache et al.\ in prep.). 

Species delimitation using marginal likelihoods in conjunction with Bayes
factors has some advantages over alternative approaches.
The flexibility of being able to compare non-nested models that contain
different numbers of species, or different species assignments, is one key
advantage.
The methods also integrate over gene trees, species trees, and other model
parameter, allowing the marginal likelihoods of delimitations to be compared
without conditioning on any parameters being known.
Marginal likelihoods also provide a natural way to rank competing models while
automatically accounting for model complexity \citep{Baele2012}.
Finally, it is unnecessary to assign prior probabilities to the alternative
species delimitation models being compared.
The marginal likelihood of a delimitation provides the factor by which the data
update our prior expectations, regardless of what that expectation is
(Equation~\ref{eq:bayesRuleModelProbability}).
As multi-species coalescent models continue to advance, using the marginal
likelihoods of delimitations will continue to be a powerful approach to
learning about biodiversity.

\section{Discussion}

\subsection{The challenge of Bayesian model choice}

It is often easy to forget about the fundamental differences between estimating
parameters by approximating the posterior distribution of a model and model
choice.
The posterior distribution of a model is informed by the likelihood function
(Equation~\ref{eq:bayesRule}),
whereas the posterior probability of that model is informed by the
\emph{marginal} likelihood
(Equation~\ref{eq:bayesRuleModelProbability}).
When we have informative data, the posterior distribution is dominated by the
likelihood, and as a result our parameter estimates are often robust to prior
assumptions we make about the parameters.
However, when comparing models, we need to assess their overall ability to
predict the data, which entails averaging over the entire parameter space of
the model, not just the regions of high likelihood.
As a result, marginal likelihoods and associated model choices can be very
sensitive to priors on the parameters of the model, even if the data are very
informative (Figure~\ref{fig:bayesDemo}).
The results of any application of Bayesian model selection should be
accompanied by an assessment of the sensitivity of those results to the priors
placed on the models' parameters.


\subsection{The state of ABC approaches to Bayesian model choice}

Given the difficulty of ABC estimation of the marginal likelihood seen here
under the simplest possible phylogenetic model
(Figure~\ref{fig:glmPerformance}), and the poor performance of ABC methods for
model selection seen elsewhere \citep{Robert2011,Oaks2012,Oaks2014reply},
Bayesian model choice based on approximate-likelihoods should be treated as a
useful means of exploring data, rather than a robust statistical framework.


\subsection{Promising future directions}

As Bayesian phylogenetics continues to explore more complex models of evolution
and datasets continue to get larger, accurate and efficient methods of
estimating marginal likelihoods will become increasingly important.
Thanks to substantial work in recent years, robust methods have been developed,
such as the generalized stepping-stone approach \citep{Fan2011}.
However, these methods are computationally demanding as they have to sample
likelihoods across a series of power-posterior distributions that are not
useful for parameter estimation.
Recent work has introduced promising methods to estimate marginal likelihoods
solely from samples from the posterior distribution.
However, these methods remain difficult to apply to phylogenetic models, and
their performance on rich models and large datasets remains to be explored.

Promising avenues for future research on methods for estimating marginal
likelihoods include continued work on reference distributions that are as
similar to the posterior as possible, but easy to formulate and use.
This would improve the performance and applicability of the GSS and derivations
of the GHM approach.
Currently, the most promising method that works solely from a posterior
sample is IDR.
Making this method easier to apply to phylogenetic models and implementing
it in popular Bayesian phylogenetic software packages,
like
RevBayes \citep{Hohna2016}
and
BEAST \citep{Drummond2012,Bouckaert2014}
would be very useful, though nontrivial.

Furthermore, nested sampling and sequential Monte Carlo are exciting new
numerical approaches to Bayesian phylogenetics.
These methods essentially use the same amount of computation to both sample
from the posterior distribution of phylogenetic models and provide an
approximation of the marginal likelihood.
Both approaches are new to phylogenetics, but hold a lot of promise
for Bayesian phylogenetics generally and model comparison via
marginal likelihoods specifically.


\subsection{Conclusions}

Marginal likelihoods are intuitive measures of model fit that are grounded in
probability theory.
As a result, they provide us with a coherent way of incrementally gaining a
better understanding about how evolution proceeds as we accrue biological data.
Because shared ancestry is a fundamental property of life, the use of marginal
likelihoods of phylogenetic models promises to continue to advance biology.
