\section{Introduction}

Phylogenetics is rapidly progressing as the statistical foundation of
comparative biology, providing a framework for accounting for the shared
ancestry inherent in biological data.
Soon after phylogenetics became feasible as a likelihood-based statistical
endeavor \citep{Felsenstein1981}, models started becoming richer to better
capture processes of biological diversification and character change.
This trend in model complexity made Bayesian approaches appealing, because they
can render inference under rich models practical by leveraging prior
information and numerical techniques (e.g., data augmentation).

From the earliest days of Bayesian phylogenetics \citep{Rannala1996,Mau1997},
the numerical tool of choice for approximating the posterior distribution was
Markov chain Monte Carlo (MCMC).
By only considering the ratios of posterior densities, MCMC obviates the need
for calculating the proportionality constant in the denominator of Bayes' rule.
This proportionality constant is better known as the marginal likelihood---the
probability of the data under the model, averaged, with respect to the prior,
over the whole parameter space.
This marginalized measure of model fit cannot be solved analytically due to
the large number of parameters in phylogenetic models that need to be summed or
integrated.

Marginal likelihoods are central to model comparison in a Bayesian framework.
Thus, we cannot avoid calculating them if we want to be able to compare the fit
of phylogenetic models.
As the diversity and richness of phylogenetic models has increased, there has
been a renewed appreciation of the importance of such Bayesian model
comparison.
As a result, there has been a lot of work over the last decade to develop
methods for estimating marginal likelihoods of phylogenetic models.

The goals of this review are to
(1) provide a summary about what marginal likelihoods are and why they are
useful,
(2) review the various methods available for approximating marginal likelihoods
of phylogenetic models,
(3) review applications
(4) \ldots

\section{What is a marginal likelihood?}

\begin{linenomath}
A marginal likelihood is the average fit of a model to a dataset.
More specifically, it is an average over the entire parameter space of the
likelihood weighted by the prior.
For a phylogenetic model \model with parameters that include the discrete
topology (\topology) and continous branch lengths and parameters that govern
the evolution of the characters along the tree (together represented by
\evoparameters), the marginal likelihood can be represented as
\begin{equation}
    p(\data \given \model) =
    \sum\limits_{\topology}
    \int_{\evoparameters}
    p( \data \given \topology, \evoparameters, \model)
    p(\topology, \evoparameters \given \model)
    \diff{\evoparameters},
    \label{eq:marginalLikelihood}
\end{equation}
where \data are the data.
Each parameter of the model adds a dimension to the model, over which the
likelihood must be averaged.
\end{linenomath}

\begin{linenomath}
The marginal likelihood is also the proportionality constant in the denominator
of Bayes' rule that ensures the posterior is a proper probability density that
sums and integrates to one:
\begin{equation}
    p(\topology, \evoparameters \given \data, \model) = \frac{
        p(\data \given \topology, \evoparameters, \model)
        p(\topology, \evoparameters \given \model)
    }{
        p(\data \given \model)
    }.
    \label{eq:bayesRule}
\end{equation}
\end{linenomath}

\section{Why are marginal likelihoods useful?}

Marginal likelihoods are the currency of model comparison in a Bayesian
framework. The frequentist approach to model choice is based on comparing
the maximum probability of the data under two models either using
a likelihood ratio test or some information-theoretic criterion.
Because adding a parameter (dimension) to a model will always ensure
a maximum likelihood at least as large as without the parameter, some
penalty must be imposed when parameters are added.

In a Bayesian perspective, we are interested in comparing the average fit of a
model, rather than the maximum.
This imposes a ``natural'' penalty for parameters, because each additional
parameter introduces a dimension that must be averaged over.
If that dimension introduces a lot of parameter space with small likelihood,
and little space that improves the likelihood, it will decrease the marginal
likelihood.
Thus, unlike the maximum likelihood, adding a parameter of a model can
decrease the \emph{marginal} likelihood.

The ratio of two marginal likelihoods gives us the factor by which the
average fit of the model in the numerator is better or worse than the
model in the denominator.
This is called the Bayes factor \citep{Jeffreys1935}, and is also equal to the factor by
which the data update the prior odds of the model.
This gives us an intuitive measure how much the data ``favor'' one model over
another.

\section{How to approximate a marginal likelihood}

For all but the simplest of models, the summation and integrals in
Equation~\ref{eq:marginalLikelihood}
are analytically intractable.
This is particularly true for phylogenetic models, which have a complex
structure containgin both discrete and continuous elements \citep{Kim2000}.
Thus, we must resort to numerical techniques to approximate the marginal
likelihood.

Perhaps the simplest numerical approximation of the marginal likelihood is to
draw samples of a model's parameters from their respective prior distributions.
This turns the intractable integral into a sum of the samples' likelihoods.
Because the prior weight of each sample is one in this case, the marginal
likelihood can be approximated by simply calculating the average likelihood of
the prior samples.
Similarly, if we have a sample of the parameters from the posterior
distribution---like one obtained from a ``standard'' Bayesian phylogenetic
analysis via MCMC---we can again use summation to approximate the integral.
The weight of each sample is the ratio of the prior density to the posterior
density.
As a result, the sum simplifies to the harmonic mean (HM) of the likelihoods
from the posterior sample \citep{Newton1994}.
Both of these techniques are importance-sampling approximations, and suffer
from the fact that the prior and posterior are often \emph{very} divergent,
with the latter usually \emph{much} more peaked than the former.
A finite sample from the prior will often yield an underestimate of the
marginal likelihood, because the region of parameter space with high likelihood
is likely to be missed.
Whereas a finite sample from the posterior will almost always lead to an
overestimate \citep{Lartillot2006,Xie2011,Fan2011}, because it will contain
very few samples outside of the region of high likelihood, where the prior has
a strong downward ``pull'' on the average likelihood.

Recent methods developed to estimate marginal likelihoods generally fall into
two categories of how to deal with the sharp contrast between the prior
and posterior that cripples the simple importance-sampling approaches
mentioned above.
One general strategy is to turn the giant leap between the unnormalized
posterior and prior into many small steps along a series of power posterior
distributions.
The second strategy is to turn the giant leap between the posterior and prior
into a smaller leap between the posterior and a reference distribution that is
as similar as possible to the posterior.
These approaches are not mutually exclusive (e.g., see Fan et al.\
\citeyear{Fan2011}), but they serve as a useful way to categorize marginal
likelihood approximation methods.
In practical terms, the first strategy is computationally expensive, because
samples need to be collected from each of the power posterior distributions,
which is not part of a standard Bayesian phylogenetic analysis.
The second strategy is very inexpensive because it attempts to approximate the
marginal likelihood using only the posterior samples collected from a
typical analysis.

\subsection{Approaches that use only posterior samples}

Gelfand and Dey \citeyear{Gelfand1994} introduced a generalized harmonic mean (GHM)
estimator that uses an arbitrary normalized reference distribution, as opposed
to the prior distribution used in the HM estimator, to weight the samples from
the posterior.
If the chosen reference distribution is more similar to the posterior than the
prior (i.e., a ``smaller leap'' as discussed above), the GHM estimator will
perform better than the HM estimator.
However, for high-dimensional phylogenetic models, choosing a suitable
reference distribution is very challenging, especially for tree topologies.
As a result, the GHM estimator has not been used for comparing phylogenetic
models.
However, recent advances on defining a reference distribution on trees
\citep{Holder2014} makes the GHM a tenable option in phylogenetics.

The inflated-density ratio estimator solves the problem of choosing a reference
distribution by using a perturbation of the posterior density; essentially the
posterior is ``inflated'' from the center by a known radius
\citep{Petris2007,Arima2012,Arima2014}.
As one might expect, the radius must be chosen carefully.
The application of this method to phylogenetics has been limited by the fact
that all parameters must be unbounded; any parameters that are bounded (e.g.,
must be positive) must be re-parameterized to span the real number line.
As a result, this method cannot be applied directy to MCMC samples collected
by popular Bayesian phylogenetic software packages.
Nonetheless, the IDR estimator had recently been applied to phylogenetic models
\citep{Arima2014}, including in settings where the topology is allowed to vary
\citep{Wu2014}.
Initial applications of the IDR are very promising, demonstrating comparable
accuracy to methods that sample from power posterior distributions while
avoiding thesuch computation \citep{Arima2014,Wu2014}.
Currently, however, the IDR has only been used on relatively small datasets and
simple models of character evolution.
More work is necessary to determine whether the promising combination of
accuracyand computational efficiency hold for large datasets and rich models.

Recently, Wang et al.\ \citeyear{Wang2017} introduced the partition weighted
kernal (PWK) method of approximating marginal likelihoods.
This approahc entails partitioning parameter space into regions within which
the posterior density is relatively homogeneous.
Given the complex structure of phylogenetic models, it is not obvious how this
would be done.
As of yet, this method has not been used for phylogenetic models.
However, for simulations for mixtures of bivariate normal distributions, the
PWK strongly outperforms the IDR estimator \citep{Wang2017}.
Thus, if this method is adapted to phylogenetic models, it is very promising.

\subsection{Approaches that use samples from power posterior distributions}

Lartillot and Philippe \citeyear{Lartillot2006} introduced a path-sampling
(also called thermodynamic integration) approach to address the problem that
the posterior is often dominated by the likelihood and very divergent from the
prior.
Rather than restrict themselves to a sample from the posterior, they collected
MCMC samples from a series of distributions between the prior and posterior.
Specifically, samples are taken from a series of power-posterior distributions,
\begin{equation}
    p(\data \given \topology, \evoparameters, \model)^{\beta}
    p(\topology, \evoparameters \given \model).
    \label{eq:powerPosterior}
\end{equation}
When $\beta = 1$, this is the unnormalized joint posterior, which integrates to
what we want to know, the marginal likelihood. When $\beta = 0$, this is the
joint prior distribution, which, assuming we are using proper prior probability
distributions, integrates to 1.
If we integrate the power posterior over 0 to 1 with respect to $\beta$, we get
the ratio of the normalizing constants when $\beta$ equals 1 and 0, and since
we know the constant is 1 when $\beta$ is zero, we are left with the marginal
likelihood.
Lartillot and Philippe \citeyear{Lartillot2006} approximated this integral by
summing over MCMC samples taken from a discrete number of $\beta$ values evenly
distributed between 1 and 0.

The stepping-stone method introduced by Xie et al.\ (\citeyear{Xie2011}) is
similar in that we also use samples from power posteriors, but the motivation
is not based on approximating the integral per se, but motivated by the fact
that we can very accurately use importance sampling to approximate the ratio of
normalizing constants at each step between the posterior and prior.
Also, Xie et al.\ (\citeyear{Xie2011}) chose the values of $\beta$ for the
series of power posteriors from which to sample so that most were close to
the prior (reference) distribution, rather than evenly distributed between
0.0 and 1.0.
This is beneficial, because this is where most of the change happens;
the likelihood begins to dominate quicky, even at small values of $\beta$
Stepping-stone method results in more accurate estimates of the marginal
likelihood with fewer steps that PS \citep{Xie2011}.

The most accurate estimator of marginal likelihoods available to date, the
generalized stepping-stone (GSS) method, leverages both strategies; taking many
small steps from a starting point (reference distribution) that is much closer
to the posterior than the prior \citep{Fan2011}.
Fan et al.\ (\citeyear{Fan2011}) improved upon the original stepping-stone
method by using a reference distribution that, in most cases, will be much more
similar to the posterior than the prior.
The reference distribution has the same form as the joint prior,
but each marginal prior
distribution is adjusted so that their means and variances match the
corresponding sample means and variances from an MCMC sample from the
posterior.
This guarantees that the support of the reference distribution will cover the
posterior.
Initially, the application of the GSS method was limited, because it required
that the topology be fixed, because there was no reference distribution across
topologies.
However, Holder et al. \citeyear{Holder2014} introduced such a reference,
allowing the GSS to approximate the fully marginalized likelihood of
phylogenetic models.

Another approach that uses sequential importance sampling steps is sequential
Monte Carlo (SMC), also known as particle filtering.
Recently, SMC algorithms have been developed for approximating the posterior
distribution of phylogenetic trees \citep{Jordan2012,Bouchard2014}.
While inferring the posterior, SMC algorithms can approximate the marginal
likelihood of the model ``for free,'' by keeping a running average of the
importance-sampling weights of the trees (particles) along the way.
SMC algorithms hold a lot of promise for complementing MCMC in Bayesian
phylogenetics due to their sequential nature an ease with which the
computations can be parallelized (see Bouchard-C\^{o}t\'{e}
\citeyear{Bouchard2014} for an accessible treatment of SMC in phylogenetics).
However, these approaches are still in their infancy, and the accuracy of SMC
estimates of marginal likelihoods still need to be compared to the methods
discussed above.

\subsection{Bayesian model averaging}

An alternative to using marginal likelihoods for Bayesian model comparison is
to sample across the competing models directly.
The frequency of samples from each model approximates their posterior
probability, which can be used to approximate Bayes factors among models.
Algorithms for sampling across models include reversible-jump MCMC
\citep{Green1995}, Gibbs sampling \citep{Neal2000}, Bayesian stochastic search
variable selection \citep{Lemey2009}, and approximations of reversible-jump
\citep{Jones2014}.
In terms of selecting the correct ``relaxed-clock'' model from simulated data,
Baele and Lemey (\citeyear{Baele2014}) showed that model-averaging performed
similarly to the path-sampling and stepping-stone marginal likelihood
estimators.

There are a couple of limitations for these approaches.
First, a Bayes factor that includes a model with small posterior probability
will suffer from Monte Carlo error.
For example, unless a very large sample from the posterior is collected, some
models might not be sampled at all.
Second, and perhaps more importantly, for these numerical algorithms to be able
to ``jump'' among models, the models being sampled need to be very similar.

If two highly dissimilar models need to be compared,
Lartillot and Philippe \citeyear{Lartillot2006} introduced a method of using
path sampling to directly approximate the Bayes factor.
Similarly, Baele et al.\ (\citeyear{Baele2013}) extended the stepping-stone
approach of Xie et al.\  (\citeyear{Xie2011}) to do the same.
However, if there are many models to compare, doing MCMC over power posteriors
for every pairwise comparison will quickly become computationally prohibitive;
approximating the marginal likelihood of each would be simpler.

\subsection{Approximate-likelihood approaches}

Approximate-likelihood Bayesian computation (ABC) approaches
\citep{Tavare1997,Beaumont2002} have become popular in situations where it is
not possible (or undesirable) to derive and compute the likelihood function of
a model.
The basic idea is simple: by generating simulations under the model, the
fraction of times that I generate a simulated dataset that matches the observed
data is a Monte Carlo approximation of the likelihood.
Because, simulating the observed data exactly is not possible (or extremely
unlikely), simulations within a zone of tolerance around the observed data are
counted (usually a set of insufficient summary statistics are used in place of
the data).

This simple approach assumes the likelihood within the zone of tolerance is
uniform.
However, this zone usually needs to be quite large for computational
tractability, so this assumption does not hold.
Leuenberger and Wegmann \citep{Leuenberger2010} proposed fitting a general
linear model (GLM) to approximate the likelihood within the zone of tolerance.
With the GLM in hand, the marginal likelihood of the model can simply be
approximated by the marginal density of the GLM.

The accuracy of this estimator has not been assessed.
However, there are good theoretical reasons to be skeptical of its accuracy.
Because the GLM is only fit within the zone of tolerance, it cannot account for
the weight of the prior on the marginal likelihood outside of this region.
Whereas the posterior distribution is usually not strongly influenced by
regions of of parameter space with low likelihood, the marginal likelihood very
much can be.
This is analogous to the harmonic mean being a poor estimator of the marginal
likelihood due to having very few samples outside of the region of high
likelihood, where the prior has a strong downward ``pull'' on the average
likelihood.


\begin{itemize}
    \item Approximate-likelihood Bayesian approaches. Wegmann's ABCToolbox
        approximates the posterior distribution with a GLM, and then takes the
        marginal of that distribution. Knowles' lab is using this to compare
        different phylogeographic models. It would be interesting to know how
        accurate these marginal likelihoods are (my guess is that they are are
        not so good). As far as I'm aware there have been no analyses to assess
        this.
        \thought{Could perform Christian Robert-esque assessment of the
            accuracy and precision of GLM approximations of the marginal
            probability of the data.}
\end{itemize}

\section{Uses of marginal likelihoods}

\subsection{Species delimitation}

Bayes factor delimitation in *BEAST and SNAPP.

\subsection{Comparing partitioning schemes}

Brandley and others used importance sampling (harmonic mean estimator) to
compare the fit of different ways to partition sequence alignments into rate
categories.

\subsection{Comparing nucleotide substitution models}

\subsection{Comparing ``relaxed clock'' models}

\subsection{Comparing tree models (priors)}

\subsection{Comparing phylogeographic models}

Knowles' lab is using Weggman's GLM approximation to compare the fit of
phylogeographic models built atop different ecological niche models.

Search for other uses of the GLM marginal.
